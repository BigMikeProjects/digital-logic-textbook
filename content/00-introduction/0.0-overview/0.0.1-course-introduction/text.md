## Introduction to Digital Logic

Digital logic is, at its core, an exercise in abstraction. We begin with physical devices whose behavior is governed by semiconductor physics, and we progressively describe them in terms that are more general and more manageable. Each level of description replaces detailed electrical behavior with a functional model that captures what matters at that scale. The discipline requires fluency in moving between these representations and understanding precisely how one level gives rise to the next.

The foundation is the transistor.

A transistor is a semiconductor device that allows one electrical signal to control the flow of current through another path. In digital systems, the transistor is engineered to operate reliably in two distinct regions: conducting and non-conducting. When used in this way, it functions as a controlled switch. Although the underlying mechanism involves continuous physical phenomena, the intended operation is discrete. Every modern digital system traces back to transistors fabricated on silicon wafers in semiconductor foundries, where billions of these switches are patterned onto a single chip.

Digital design begins by assigning meaning to these discrete states. A range of voltages is defined to represent logical 0, and another range represents logical 1. This convention is not merely a labeling choice; it is the basis for building systems whose behavior can be described symbolically. On an oscilloscope, a digital signal appears as a waveform switching between two well-defined voltage levels, the physical manifestation of abstract binary values. Once voltage levels are interpreted as binary values, the analysis of circuits can be carried out using Boolean algebra rather than device physics.

Transistors are arranged into standard configurations to form logic gates such as AND, OR, and NOT. Each gate implements a Boolean function, producing an output that depends only on its current inputs. At this level, the designer no longer reasons about charge carriers or doping profiles. Instead, circuit behavior is represented using truth tables, logic expressions, and timing relationships. The physical implementation remains essential, but it is hidden behind a functional description.

Logic gates are then combined to form combinational circuits. These circuits implement useful functions such as selection, decoding, comparison, and arithmetic. A multiplexer chooses one of several inputs. A decoder activates one of many outputs corresponding to a binary code. Adders implement binary arithmetic by coordinating the generation and propagation of carries. These circuits may contain many gates, but they remain analyzable because their behavior is determined entirely by their inputs at a given moment. There is no dependence on history.

Many digital systems, however, require memory. Computation often involves intermediate results. Control systems must remember the current stage of operation. Communication systems must track sequences over time. These requirements lead to sequential logic, in which circuit behavior depends not only on present inputs but also on stored state.

The most common memory element is the flip-flop. A flip-flop stores one bit of information and changes state only under controlled conditions, typically synchronized to a clock signal. With the introduction of memory, time becomes an explicit part of the design model. The system's response is no longer a direct function of its inputs alone; it also depends on what the system has previously stored.

By combining combinational logic with flip-flops, we construct registers, counters, and finite state machines. Registers store multi-bit values. Counters generate predictable sequences. Finite state machines describe systems that transition among a defined set of states according to input conditions. A familiar example is a traffic light controller, which cycles through red, yellow, and green states in response to timers and sensor inputs. Many controllers in digital systems can be described naturally in this framework.

As these components are organized further, they form structured subsystems. Arithmetic units combine adders, shifters, and multiplexers to manipulate data. Register files provide organized storage. Control units coordinate sequencing and operation selection. Together, these subsystems form processors capable of executing instruction sets. At larger scales, processors interact with memory systems and input/output interfaces to form complete computing platforms. The cumulative effect of these abstractions is evident in modern data centers, where racks of processors built from the same transistor-level foundations deliver computing performance that continues to grow dramatically.

Throughout this progression, abstraction is not optional; it is the mechanism that makes design possible. At one moment, it may be necessary to reason at the transistor level in order to understand timing or power behavior. At another, it is more appropriate to reason using Boolean expressions or state diagrams. Effective design depends on selecting the correct level of description for the problem at hand and maintaining clear connections between levels.

This text follows that progression. It begins with transistors and CMOS logic, then develops Boolean algebra as a tool for analysis and design. It introduces combinational circuits and arithmetic structures, followed by memory elements and synchronous design principles. Finally, it examines structured digital systems built from these components.

The goal is to establish a clear and practical connection between physical devices and the organized systems that perform computation. With these foundations in place, more advanced topics in computer engineering can be approached with clarity and precision.
